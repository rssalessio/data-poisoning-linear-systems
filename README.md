# data-poisoning-linear-systems
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Accompanying code for the paper _Analysis and Detectability of Offline Data Poisoning Attacks on Linear Systems_.

Author: Alessio Russo (alessior@kth.se)
License: MIT

## Input poisoning attack

To run the example, run the file `main.py` in the foler `example_input_attack`. Feel free to change the standard deviation of the process noise `std_w`, or of the input signal `std_u`.

![Input poisoning](example_input_attack/figures/input_poisoning_1.pdf "Scalar system")

## Residuals maximization attack

## Stealthy attack

